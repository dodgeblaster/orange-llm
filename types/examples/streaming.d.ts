/**
 * Streaming Example
 *
 * This example demonstrates how to use the LLM module with streaming responses.
 * It shows how to:
 * 1. Create an LLM service using the factory
 * 2. Set up streaming for responses
 * 3. Process chunks of the response as they arrive
 *
 * Note: This example assumes the LLMService interface has streaming capabilities.
 * If the actual implementation doesn't support streaming, this example would need
 * to be modified accordingly.
 */
export {};
