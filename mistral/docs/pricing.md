# Premier Models

## Mistral Medium 3
State-of-the-art performance. Simplified enterprise deployments. Cost-efficient.

Input (/M tokens)
$0.4

Output (/M tokens)
$2

mistral-medium-latest

## Magistral Medium (Preview)
Thinking model excelling in domain-specific, transparent, and multilingual reasoning.

Input (/M tokens)
$2

Output (/M tokens)
$5

magistral-medium-latest

## Codestral
Lightweight, fast, and proficient in over 80 programming languages.

Input (/M tokens)
$0.3

Output (/M tokens)
$0.9

codestral-latest

## Mistral Saba
Custom-trained model to serve specific geographies, markets, and customers.
Input (/M tokens)
$0.2

Output (/M tokens)
0.6$

mistral-saba-latest

## Mistral Large
Top-tier reasoning for high-complexity tasks and sophisticated problems.
Input (/M tokens)
$2

Output (/M tokens)
$6

mistral-large-latest

## Ministral 8B 24.10
Powerful model for on-device use cases.
Input (/M tokens)
$0.1

Output (/M tokens)
$0.1

ministral-8b-latest

## Ministral 3B 24.10
Most efficient edge model.
Input (/M tokens)
$0.04

Output (/M tokens)
$0.04

ministral-3b-latest


# Open Models

## Mistral Small 3.1
SOTA. Multimodal. Multilingual. Apache 2.0.
Input (/M tokens)
$0.1

Output (/M tokens)
$0.3

mistral-small-latest

## Magistral Small
Thinking model excelling in domain-specific, transparent, and multilingual reasoning.

Input (/M tokens)
$0.5

Output (/M tokens)
$1.5

magistral-small-latest

## Devstral
The best open-source model for coding agents.

Input (/M tokens)
$0.1

Output (/M tokens)
$0.3

devstral-small-2505

## Mistral NeMo
State-of-the-art Mistral model trained specifically for code tasks.
Input (/M tokens)
$0.15

Output (/M tokens)
$0.15

mistral-nemo

## Mistral 7B
A 7B transformer model, fast-deployed and easily customisable.
Input (/M tokens)
$0.25

Output (/M tokens)
$0.25

open-mistral-7b

## Mixtral 8x7B
A 7B sparse Mixture-of-Experts (SMoE). Uses 12.9B active parameters out of 45B total.
Input (/M tokens)
$0.7

Output (/M tokens)
$0.7

open-mixtral-8x7b

## Mixtral 8x22B
Mixtral 8x22B is currently the most performant open model. A 22B sparse Mixture-of-Experts (SMoE). Uses only 39B active parameters out of 141B.
Input (/M tokens)
$2

Output (/M tokens)
$6

open-mixtral-8x22b